training {
  task = "ner"
  dataset = "conll04"

  data_dir = /home/flo/ContaminedNER/datasets/conll04
  log_dir = "log_data"
  local_dir = "training_data"

  types_path = /home/flo/ContaminedNER/datasets/conll04/conll04_types.json
  train_path = /home/flo/ContaminedNER/datasets/conll04/conll04_conta10_split4_train.json
  test_path = /home/flo/ContaminedNER/datasets/conll04/conll04_test.json
  test_path_seen = /home/flo/ContaminedNER/datasets/conll04/conll04_conta10_split4_seen_test.json
  test_path_unseen = /home/flo/ContaminedNER/datasets/conll04/conll04_conta10_split4_unseen_test.json
  dev_path = /home/flo/ContaminedNER/datasets/conll04/conll04_dev.json

  max_segment_len = 256

  # Learning
  use_amp = true
  optimizer = "adamw"
  plm_learning_rate = 3e-5
  task_learning_rate = 3e-4
  plm_scheduler = "linear_with_warmup" # constant / constant_with_warmup / linear_with_warmup
  task_scheduler = "linear_with_warmup"

  warmup_ratio = 0.05

  adam_eps = 1e-8
  adam_weight_decay = 0.1

  init_std = 0.02
  max_grad_norm = 1  # Set 0 to disable clipping

  batch_size = 8
  gradient_accumulation_steps = 1
  num_epochs = 100

  # Model hyperparameters.
  activation = "relu"
  dropout_rate = 0.3
  feature_emb_size = 20
  hidden_size = 1500

  # number of types
  num_typing_classes = 4
  num_linking_classes = 5

  # Other.
  beam_size = 1

  eval_frequency = 500
  report_frequency = 20

  plm_tokenizer_name = t5-small
}

t5-small = ${training}{
  plm_learning_rate = 5e-5
  task_learning_rate = 1e-4

  plm_pretrained_name_or_path = t5-small
}

t5-base = ${training}{
  plm_learning_rate = 5e-5
  task_learning_rate = 1e-4

  plm_pretrained_name_or_path = t5-base
}
google_flan-t5-base = ${training}{
  plm_learning_rate = 5e-5
  task_learning_rate = 1e-4

  plm_pretrained_name_or_path = google/flan-t5-base
}

t5-large = ${training}{
  plm_learning_rate = 5e-5
  task_learning_rate = 3e-4

  plm_pretrained_name_or_path = t5-large
}
google_flan-t5-large = ${t5-large}{
  plm_pretrained_name_or_path = google/flan-t5-large
}

t5-3b = ${t5-large}{
  plm_learning_rate = 3e-5
  task_learning_rate = 3e-4

  plm_pretrained_name_or_path = t5-3b
}
google_flan-t5-xl = ${t5-3b}{
  plm_pretrained_name_or_path = google/flan-t5-xl
}
t0-3b = ${t5-3b}{
  plm_pretrained_name_or_path = bigscience/T0_3B
}
