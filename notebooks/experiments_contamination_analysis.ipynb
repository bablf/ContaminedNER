{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Initialize functions for loading metrics files.",
   "id": "a370090e663d5667"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "if not os.getcwd().endswith(\"ContaminedNER\"):\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "print(os.getcwd())\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "from notebooks.utils import (get_dataset)\n",
    "%matplotlib inline\n",
    "\n",
    "datasets = [\"scierc\", \"ace05\", \"conll04\", \"genia\", \"nyt\", ]\n",
    "\n",
    "def rename_diff_metric(metric):\n",
    "    kind = \"\" if \"micro\" in metric else \"macro\"\n",
    "    if \"ner_prec_\" in metric:\n",
    "        if kind == \"\":  # micro\n",
    "            return f\"ner_pr\"\n",
    "        return f\"macro_ner_pr\"\n",
    "    elif \"ner_rec_\" in metric:\n",
    "        if kind == \"\":  # micro\n",
    "            return f\"ner_rec\"\n",
    "        return f\"macro_ner_rec\"\n",
    "    elif \"ner_f1_\" in metric:\n",
    "        if kind == \"\":  # micro\n",
    "            return f\"ner_f1\"\n",
    "        return f\"macro_ner_f1\"\n",
    "    else:  # classname\n",
    "        if kind == \"\":  # micro\n",
    "            return metric.strip(\"_micro\")\n",
    "        #macro\n",
    "        return \"macro_\" + metric.strip(\"_macro\")\n",
    "\n",
    "\n",
    "def rename_asp_metric(metric):\n",
    "    if \"Precision\" in metric:\n",
    "        return \"ner_pr\"\n",
    "    elif \"Recall\" in metric:\n",
    "        return \"ner_rec\"\n",
    "    elif \"F1\" in metric:\n",
    "        return \"ner_f1\"\n",
    "    else:\n",
    "        print(\"SHOULD NOT HAPPEN\")\n",
    "\n",
    "\n",
    "def load_asp_diffner_metric_files(data_dir, model=\"google_flan-t5-base\", model_name=\"asp\", dataset=None):\n",
    "    metrics_data = []\n",
    "    # if path does not exist skip:\n",
    "    for c in range(0, 110, 10):\n",
    "        for s in range(0, 5):\n",
    "            filename = data_dir + f\"cfg_paper_dataset_contamination_{dataset}_dataset_contamination_{model_name}_{dataset}_conta{c}/{model}/000{s}/metrics.json\"\n",
    "            if not os.path.exists(filename):\n",
    "                if model_name==\"asp\" and dataset == \"genia\":\n",
    "                    continue\n",
    "                print(f\"{model_name}: {dataset}, conta {c}, split {s}, does not exist, {filename}\")\n",
    "                continue\n",
    "            metrics = json.load(open(filename))\n",
    "            tmp_metrics = defaultdict(dict)\n",
    "            for metric, score in metrics[\"test_metrics\"].items():\n",
    "                if model_name == \"diffusion\":\n",
    "                    if \"_prec_\" not in metric and \"_rec_\" not in metric and \"_f1_\" not in metric:\n",
    "                        continue\n",
    "                    if metric.startswith(\"unseen\"):\n",
    "                        m = rename_diff_metric(metric.split(\"_\", maxsplit=1)[1])\n",
    "                        tmp_metrics[\"unseen\"][m] = round(score / 100, 4)\n",
    "                    elif metric.startswith(\"seen\"):\n",
    "                        m = rename_diff_metric(metric.split(\"_\", maxsplit=1)[1])\n",
    "                        tmp_metrics[\"seen\"][m] = round(score / 100, 4)\n",
    "                    else:\n",
    "                        m = rename_diff_metric(metric)\n",
    "                        tmp_metrics[\"normal\"][m] = round(score / 100, 4)\n",
    "\n",
    "                else:\n",
    "                    if \"Precision\" not in metric and \"Recall\" not in metric and \"F1\" not in metric:\n",
    "                        continue\n",
    "                    m = rename_asp_metric(metric)\n",
    "                    if metric.startswith(\"Eval\"):\n",
    "                        tmp_metrics[\"normal\"][m] = round(score / 100, 4)\n",
    "                    elif metric.startswith(\"seen\"):\n",
    "                        tmp_metrics[\"seen\"][m] = round(score / 100, 4)\n",
    "                    if metric.startswith(\"unseen\"):\n",
    "                        tmp_metrics[\"unseen\"][m] = round(score / 100, 4)\n",
    "\n",
    "            for testvalue in [\"normal\", \"seen\", \"unseen\"]:\n",
    "                metrics_data.append({\n",
    "                    \"dataset\": dataset,\n",
    "                    \"split\": int(s),\n",
    "                    \"test\": testvalue,\n",
    "                    \"model\": model_name,\n",
    "                    \"percent\": int(c),\n",
    "                    \"metrics\": tmp_metrics[testvalue],\n",
    "                })\n",
    "\n",
    "    return metrics_data\n",
    "\n",
    "def load_metrics_files(parent_folder, dataset_name, model_name=\"iter\"):\n",
    "    metrics_data = []\n",
    "    for prefix in [\"\", \"_%SPLIT%split%PERCENT%percentSeenUnseen\"]:  # it just works ok.\n",
    "        for folder_name in os.listdir(parent_folder):\n",
    "            folder_path = os.path.join(parent_folder, folder_name)\n",
    "            if not os.path.isdir(folder_path) and \"split\" not in folder_path:\n",
    "                continue\n",
    "            try:\n",
    "                split_value = folder_name.split('split')[0]\n",
    "                percent_value = folder_name.split('split')[1].replace('percent', '')\n",
    "            except IndexError:\n",
    "                print(f\"Skipping folder with unexpected name: {folder_name}\")\n",
    "                continue\n",
    "            if prefix == \"_%SPLIT%split%PERCENT%percentSeenUnseen\":  #metrics_0split40percentSeenUnseen\n",
    "                tmp = prefix.replace(\"%SPLIT%\", split_value).replace(\"%PERCENT%\", percent_value)\n",
    "                metrics_file = os.path.join(folder_path, f\"metrics{tmp}.json\")  # metrics_entitiesPurified.json\n",
    "            else:\n",
    "                metrics_file = os.path.join(folder_path, f\"metrics{prefix}.json\")  # metrics_entitiesPurified.json\n",
    "\n",
    "            if not os.path.exists(metrics_file):\n",
    "                print(f\"No metrics {metrics_file} file found in folder: {folder_name}\")\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "            if os.path.exists(metrics_file):\n",
    "                with open(metrics_file, \"r\") as f:\n",
    "                    metrics = json.load(f)\n",
    "                if prefix == \"\":\n",
    "                    testvalue = \"normal\"\n",
    "                    metrics_data.append({\n",
    "                        \"dataset\": dataset_name,\n",
    "                        \"split\": int(split_value),\n",
    "                        \"test\": testvalue,\n",
    "                        \"model\": model_name,\n",
    "                        \"percent\": int(percent_value),\n",
    "                        \"metrics\": metrics[\"test_metrics\"],\n",
    "                    })\n",
    "                else:\n",
    "                    tmp_metrics = defaultdict(dict)\n",
    "                    for metric, s in metrics[\"test_metrics\"].items():\n",
    "                        if metric.startswith(\"seen__\"):\n",
    "                            m = metric.replace(\"seen__\", \"\")\n",
    "                            tmp_metrics[\"seen\"][m] = s\n",
    "                        elif metric.startswith(\"unseen__\"):\n",
    "                            m = metric.replace(\"unseen__\", \"\")\n",
    "                            tmp_metrics[\"unseen\"][m] = s\n",
    "\n",
    "                    for testvalue in [\"seen\", \"unseen\"]:\n",
    "                        metrics_data.append({\n",
    "                            \"dataset\": dataset_name,\n",
    "                            \"split\": int(split_value),\n",
    "                            \"test\": testvalue,\n",
    "                            \"model\": model_name,\n",
    "                            \"percent\": int(percent_value),\n",
    "                            \"metrics\": tmp_metrics[testvalue],\n",
    "                        })\n",
    "    return metrics_data\n",
    "\n"
   ],
   "id": "e65a02d7fe691d13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def get_enttype_split_counts(path2dir: Path):\n",
    "    rows = []\n",
    "    entity_sample_counts = []\n",
    "    for dataset in tqdm.tqdm(datasets):\n",
    "        for s in range(0, 5):\n",
    "            for c in range(0, 110, 10):\n",
    "                split2counts = dict()\n",
    "                for dataset_split in [\"train\", \"test\", \"seen\", \"unseen\"]:\n",
    "                    if dataset_split == \"train\":\n",
    "                        filename = path2dir / Path(dataset) / Path(f\"{dataset}_conta{c}_split{s}_{dataset_split}.json\")\n",
    "                    elif dataset_split == \"test\":\n",
    "                        filename = path2dir / Path(dataset) / Path(f\"{dataset}_{dataset_split}.json\")\n",
    "                    else:\n",
    "                        filename = path2dir / Path(dataset) / Path(\n",
    "                            f\"{dataset}_conta{c}_split{s}_{dataset_split}_test.json\")\n",
    "                    for enttype, count in Counter([ent[\"type\"].lower() for example in json.load(open(os.path.abspath(filename), \"r\")) for ent in\n",
    "                                                   example[\"entities\"]]).items():\n",
    "                        rows.append({\n",
    "                            \"dataset\": dataset,\n",
    "                            \"split\": s,\n",
    "                            \"conta_percent\": c,\n",
    "                            \"dataset_split\": dataset_split,\n",
    "                            \"ent_type\": enttype,\n",
    "                            \"count\": count,\n",
    "                        })\n",
    "                    if dataset_split in [\"test\", \"seen\", \"unseen\"]:\n",
    "                        split2counts[dataset_split] = 0\n",
    "                        for example in json.load(open(filename, \"r\")):\n",
    "                            split2counts[dataset_split] += len(example[\"entities\"])\n",
    "\n",
    "                entity_sample_counts.append({\n",
    "                    \"dataset\": dataset,\n",
    "                    \"split\": s,\n",
    "                    \"conta_percent\": c,\n",
    "                    \"E_orig\": split2counts[\"test\"],\n",
    "                    \"E_conta\": split2counts[\"seen\"],\n",
    "                    \"E_clean\": split2counts[\"unseen\"],\n",
    "                })\n",
    "    return pd.DataFrame(rows).drop_duplicates(), pd.DataFrame(entity_sample_counts),\n",
    "\n",
    "\n",
    "count_df, entity_sample_counts = get_enttype_split_counts(path2dir=Path(\"datasets/\"))"
   ],
   "id": "3c7cfd04625d8c4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Load experiment files into dataframes\n",
   "id": "e1a1b3dab08dede4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dfs = []\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    with open(f\"results/experiment_results/{dataset}_split_stats.json\", \"r\") as f:\n",
    "        data_from_json = json.load(f)\n",
    "\n",
    "    # Flattening the nested dictionary into a DataFrame\n",
    "    rows = []\n",
    "    for percentage, splits in data_from_json.items():\n",
    "        for split, values in splits.items():\n",
    "            rows.append({\n",
    "                \"conta_percent\": int(percentage),\n",
    "                \"split\": int(split),\n",
    "                \"ent_conta_percent\": values[\"ent_conta_rate\"],\n",
    "                \"num_entities\": values[\"num_entities\"]\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if os.path.exists(f\"results/experiment_results/iter/{dataset}/0split0percent/\"):\n",
    "        iter_metrics = load_metrics_files(f\"results/experiment_results/iter/{dataset}/\", dataset)\n",
    "        asp_metrics = load_asp_diffner_metric_files(f\"results/experiment_results/asp/\", dataset=dataset)\n",
    "        diff_metrics = load_asp_diffner_metric_files(f\"results/experiment_results/diffusionner/\",\n",
    "                                                     model=\"bert-base-cased\", model_name=\"diffusion\", dataset=dataset)\n",
    "        metrics_data = iter_metrics + asp_metrics + diff_metrics\n",
    "        metric_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "        df_expanded = metric_df.join(pd.json_normalize(metric_df['metrics']).dropna(subset=[\"ner_f1\"]))\n",
    "        df_expanded.drop(columns=['metrics'], inplace=True)\n",
    "        df = pd.merge(df, df_expanded, how=\"inner\", left_on=[\"split\", \"conta_percent\"],\n",
    "                      right_on=[\"split\", \"percent\"]).drop(columns=[\"percent\"])\n",
    "        df.dropna(axis=1, how='all', inplace=True)\n",
    "    else:\n",
    "        print(f\"Skipping {dataset} split\")\n",
    "    tmp = pd.merge(df[df.test==\"normal\"], df[df.test==\"seen\"], how=\"inner\", on=[\"model\", \"dataset\", \"split\", \"conta_percent\"], suffixes=(\"\", \"_seen\"))\n",
    "    df = tmp.merge(df[df.test==\"unseen\"], how=\"inner\", on=[\"model\", \"dataset\", \"split\", \"conta_percent\"], suffixes=(\"\", \"_unseen\"))\n",
    "    # Replace asp values for seen and conta == 0 because outputs are all 0.5 and thats false.\n",
    "    df.loc[(df.model==\"asp\") & (df.conta_percent==0), \"ner_rec_seen\"] = 0.0\n",
    "    # calcualting ner_f1 seen unseen\n",
    "    df[\"ner_f1_unseen\"] = (2* df[\"ner_pr\"] * df[\"ner_rec_unseen\"]) / (df[\"ner_pr\"] + df[\"ner_rec_unseen\"])\n",
    "    df[\"ner_f1_seen\"] = (2* df[\"ner_pr\"] * df[\"ner_rec_seen\"]) / (df[\"ner_pr\"] + df[\"ner_rec_seen\"])\n",
    "    df[\"ner_f1_delta\"] = df[\"ner_f1\"] - df[\"ner_f1_unseen\"]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Create one dataframe out of all\n",
    "entire_df = pd.concat(dfs).reset_index(drop=True)\n",
    "entire_df[\"ner_f1_delta\"] = entire_df[\"ner_f1\"] - entire_df[\"ner_f1_unseen\"]\n",
    "# make all scores xx.yy format:\n",
    "entire_df[[c for c in entire_df if \"f1\" in c or \"_pr\" in c or \"_rec\" in c ]] = entire_df[[c for c in entire_df if \"f1\" in c or \"_pr\" in c or \"_rec\" in c ]] * 100\n",
    "entire_df\n",
    "#results/experiment_results/asp/cfg_paper_dataset_contamination_ace05_dataset_contamination_asp_ace05_conta70/google_flan-t5-base/0004/metrics.json"
   ],
   "id": "89444037ef9c415c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "fontsize = 12\n",
    "plt.figure(figsize=(6, 5.25), dpi=100)\n",
    "sns.scatterplot(data=entire_df, x='conta_percent', y='ent_conta_percent', hue='dataset', style='dataset',\n",
    "                s=110, alpha=0.9)\n",
    "plt.title('Named Entity Contamination Rate to Sample Contamination Rate', fontsize=11, loc='right', )\n",
    "plt.xlabel('Sample Contamination Rate (%)', fontsize=fontsize)\n",
    "plt.xticks(sorted(entire_df['conta_percent'].unique()), fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.ylabel('Named Entity Contamination Rate (%)', fontsize=fontsize)\n",
    "plt.legend(title=\"Dataset\")\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.ylim(-4, 85)\n",
    "plt.savefig('plots/NEC2CR.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 5.25), dpi=100)\n",
    "sns.scatterplot(data=entire_df, x='conta_percent', y='num_entities', hue='dataset', s=130, style='dataset',\n",
    "                legend=False)\n",
    "plt.title('Sample Contamination Rate to Number of Entities', fontsize=fontsize, loc='right', )\n",
    "plt.xlabel('Sample Contamination Rate (%)', fontsize=fontsize)\n",
    "# Rotate x-axis labels if there are many values\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.xlim(-3, 82)\n",
    "plt.ylabel('Number of Entities (logscale)', fontsize=fontsize)\n",
    "plt.yscale(\"log\")\n",
    "# plt.legend(title=\"Dataset\", loc=\"lower left\", bbox_to_anchor=(-0.25, -0),)\n",
    "y_ticks = [1000, 5000, 10000, 26000]\n",
    "plt.yticks(y_ticks, [f'{int(x / 1000):,}k' for x in y_ticks], fontsize=fontsize)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "plt.savefig('plots/NEC2numEnts.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "4a6446955b0a34b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#####  Plot lineplots",
   "id": "82e54fc18b0f974f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset2contamination = {\n",
    "    \"scierc\": 0.50,\n",
    "    \"nyt\": 0.99,\n",
    "    \"genia\": 0.735,\n",
    "    \"conll04\": 0.823,\n",
    "    \"ace05\": 0.701,\n",
    "    \"ade\": 0.3309\n",
    "}\n",
    "color_mapping = {\n",
    "    'original': \"black\",\n",
    "    'normal': 'magenta',\n",
    "    'seen':  \"purple\",\n",
    "    'unseen': \"limegreen\",\n",
    "}\n",
    "dataset_name_mapping = {\"genia\": \"GENIA\", \"nyt\": \"NYT\", \"ace05\": \"ACE05\", \"conll04\": \"CoNLL04\", \"scierc\": \"SciERC\"}\n",
    "model_name_mapping = {\"iter\": \"ITER\", \"asp\": \"ASP\", \"diffusion\": \"DiffusionNER\"}\n",
    "n_datasets = len(datasets)\n",
    "fig, axs = plt.subplots(n_datasets, 3, figsize=(19, 5 * n_datasets), sharex='row')\n",
    "value_vars = [\"ner_f1\", \"ner_f1_seen\", \"ner_f1_unseen\"]\n",
    "\n",
    "for i, dataset_name in enumerate(datasets):\n",
    "    df = entire_df[entire_df['dataset'] == dataset_name]\n",
    "    print(dataset_name)\n",
    "    def split_test(x):\n",
    "        if x.split(\"_\")[-1] == \"f1\":\n",
    "            return \"original\"\n",
    "        if x.split(\"_\")[-1] == \"seen\":\n",
    "            return \"seen\"\n",
    "        if x.split(\"_\")[-1] == \"unseen\":\n",
    "            return \"unseen\"\n",
    "\n",
    "    sns.set_style('ticks')\n",
    "    for j, model in enumerate([\"iter\", \"asp\", \"diffusion\"]):\n",
    "        # Add a new column 'test' using the mapping.\n",
    "        tmp = pd.melt(df,\n",
    "                      id_vars=[\"conta_percent\", \"split\", \"model\", \"dataset\"],\n",
    "                      value_vars=value_vars,\n",
    "                      var_name=\"metric\",\n",
    "                      value_name=\"value\")\n",
    "        tmp['test'] = tmp['metric'].map(split_test)\n",
    "        filter = (tmp.model == model) & (tmp[\"metric\"].isin(value_vars)) & ~((tmp.test ==\"seen\") & (tmp.conta_percent == 0))\n",
    "        model_data = tmp[filter]\n",
    "        # Compute y_min and y_max for the current dataset\n",
    "        all_values = tmp[filter]['value'].dropna()\n",
    "        y_min, y_max = max(all_values.min() * 0.9, 0.4), all_values.max() * 1.1\n",
    "        x_vals = list(range(0, 120, 20))  # Define x_vals here as per original code\n",
    "\n",
    "        if model_data.shape[0] == 0 :\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            axs[i, j].legend(\n",
    "                handles,\n",
    "                [\"Original Contamination\", \"original\", \"contaminated\", \"clean\"],\n",
    "                title='Test Split',\n",
    "                loc='lower center',\n",
    "                bbox_to_anchor=(0.53, 0.3),\n",
    "                ncol=1,\n",
    "                fontsize=17\n",
    "            )\n",
    "\n",
    "            axs[i, j].legend_.get_frame().set_alpha(None)\n",
    "            axs[i, j].legend_.get_frame().set_facecolor((1, 1, 1, 0.1))\n",
    "            axs[i,j].grid(False)\n",
    "\n",
    "            continue\n",
    "        ax_index = (i,j)\n",
    "        ax = axs[ax_index]\n",
    "        contamination = dataset2contamination[dataset_name] * 100\n",
    "        ax.axvline(\n",
    "            x=contamination,\n",
    "            color='black',\n",
    "            ls='--',\n",
    "            label='Original Contamination'\n",
    "        )\n",
    "        sns.lineplot(\n",
    "            data=model_data,\n",
    "            x=\"conta_percent\",\n",
    "            y=\"value\",\n",
    "            hue=\"test\",\n",
    "            palette=color_mapping,\n",
    "            errorbar=('sd', 1),\n",
    "            ax=ax,\n",
    "            legend=True,\n",
    "        )\n",
    "        ax.set_xticks(x_vals)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.grid(True)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel('F1', fontsize=17)\n",
    "        else:\n",
    "            ax.set_ylabel('', fontsize=0)\n",
    "\n",
    "        #ax.set_ylim(59 + 0.05, 100 - 0.05)\n",
    "        ax.set_xlim(0,100)\n",
    "        if ax.get_legend() is not None:\n",
    "            ax.get_legend().remove()\n",
    "        # if model == \"diffusion\" and dataset_name == \"ace05\":\n",
    "        #     handles, labels = ax.get_legend_handles_labels()\n",
    "        #     ax.legend(handles, [\"Original Contamination\", \"original\", \"contam.\", \"clean\"],bbox_to_anchor=(-1.6, -0.3), loc='upper left', fontsize=17, ncol=4)\n",
    "        # else:\n",
    "        #     ax.get_legend().remove()\n",
    "        if i == len(datasets) - 1:\n",
    "            ax.set_xlabel('Train Contamination (%)', fontsize=17)\n",
    "        else:\n",
    "            ax.set_xlabel('', fontsize=0)\n",
    "\n",
    "        ax.set_title(model_name_mapping[model], fontsize=17)\n",
    "\n",
    "    # Set dataset name as title for the first subplot of the row\n",
    "    axs[i, 1].set_title(f\"Dataset: {dataset_name_mapping[dataset_name]}\\n{axs[i, 1].get_title()}\", fontsize=17, pad=10)\n",
    "fig.subplots_adjust(top=0.95, hspace=0.5, wspace=0.2)\n",
    "sns.despine()\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"plots/model2dataset2seenUnseenNormalPerContaminationNYT.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "25fc82414653acbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"text.usetex\"] = True\n",
    "mpl.rcParams[\"text.latex.preamble\"] = (\n",
    "    r\"\\usepackage{amsmath}\"\n",
    "    r\"\\usepackage[usenames,dvipsnames]{color}\"\n",
    "    r\"\\usepackage[dvipsnames]{xcolor}\"\n",
    "    r\"\\usepackage{amssymb}\"\n",
    ")\n",
    "\n",
    "dataset2contamination = {\n",
    "    \"scierc\": 0.50,\n",
    "    \"nyt\": 0.99,\n",
    "    \"genia\": 0.735,\n",
    "    \"conll04\": 0.823,\n",
    "    \"ace05\": 0.701,\n",
    "    \"ade\": 0.3309\n",
    "}\n",
    "color_mapping = {\n",
    "    'original': \"black\",\n",
    "    'Original Testset': \"black\",\n",
    "    'normal': '#ed6e00',\n",
    "    'seen':  \"#7b4173\",\n",
    "    'Contaminated Entities':  \"#7b4173\",\n",
    "    'unseen': \"#ed6e00\",\n",
    "    'Clean Entities': \"#ed6e00\",\n",
    "}\n",
    "dataset_name_mapping = {\"genia\": \"GENIA\", \"nyt\": \"NYT\", \"ace05\": \"ACE05\", \"conll04\": \"CoNLL04\", \"scierc\": \"SciERC\"}\n",
    "model_name_mapping = {\"iter\": \"ITER\", \"asp\": \"ASP\", \"diffusion\": \"DiffusionNER\"}\n",
    "plot_datasets = [(\"nyt\", \"diffusion\"), (\"ace05\", \"iter\")]\n",
    "n_datasets = len(plot_datasets)\n",
    "fig, axs = plt.subplots(1, n_datasets, figsize=(12.4, 4.8), sharex=\"row\", layout=\"constrained\")\n",
    "value_vars = [\"ner_f1\", \"ner_f1_seen\", \"ner_f1_unseen\"]\n",
    "\n",
    "for i, (dataset_name, model) in enumerate(plot_datasets):\n",
    "    df = entire_df[entire_df['dataset'] == dataset_name]\n",
    "    print(dataset_name)\n",
    "    def split_test(x):\n",
    "        if x.split(\"_\")[-1] == \"f1\":\n",
    "            return \"original\"\n",
    "        if x.split(\"_\")[-1] == \"seen\":\n",
    "            return \"seen\"\n",
    "        if x.split(\"_\")[-1] == \"unseen\":\n",
    "            return \"unseen\"\n",
    "\n",
    "    sns.set_style('ticks')\n",
    "    if True:\n",
    "        # Add a new column 'test' using the mapping.\n",
    "        tmp = pd.melt(df,\n",
    "                      id_vars=[\"conta_percent\", \"split\", \"model\", \"dataset\"],\n",
    "                      value_vars=value_vars,\n",
    "                      var_name=\"metric\",\n",
    "                      value_name=\"value\")\n",
    "        tmp['test'] = tmp['metric'].map(split_test)\n",
    "        filter = (tmp.model == model) & (tmp[\"metric\"].isin(value_vars)) & ~((tmp.test ==\"seen\") & (tmp.conta_percent == 0))\n",
    "        model_data = tmp[filter]\n",
    "        # Compute y_min and y_max for the current dataset\n",
    "        all_values = tmp[filter]['value'].dropna()\n",
    "        y_min, y_max = max(all_values.min() * 0.9, 0.4), all_values.max() * 1.1\n",
    "        x_vals = list(range(0, 120, 20))  # Define x_vals here as per original code\n",
    "\n",
    "        ax = axs[i]\n",
    "        contamination = dataset2contamination[dataset_name] * 100\n",
    "        ax.axvline(\n",
    "            x=contamination,\n",
    "            color='black',\n",
    "            ls='--',\n",
    "            label='Original Contamination'\n",
    "        )\n",
    "        sns.lineplot(\n",
    "            data=model_data.assign(test=model_data[\"test\"].map({'original': 'Original Testset', 'seen': 'Contaminated Entities', 'unseen': 'Clean Entities'})),\n",
    "            x=\"conta_percent\",\n",
    "            y=\"value\",\n",
    "            hue=\"test\",\n",
    "            palette=color_mapping,\n",
    "            errorbar=('sd', 1),\n",
    "            ax=ax,\n",
    "            legend=True,\n",
    "        )\n",
    "\n",
    "        ax.spines[\"bottom\"].set_linewidth(2)\n",
    "        ax.spines[\"left\"].set_linewidth(2)\n",
    "        ax.set_xticks(x_vals, [rf\"\\textbf{{{x_val}\\%}}\" for x_val in x_vals])\n",
    "        # yticks = []\n",
    "        # assert False, ax.get_yticks()\n",
    "        # ax.set_yticks(ax.get_yticks(), [rf\"\\textbf{{{x_val}}}\" for x_val in ax.get_yticks()])\n",
    "        ax.set_ylim(70, 93.5)\n",
    "        ax.set_yticks([lbl._y for lbl in ax.get_yticklabels()], [rf\"\\textbf{{{lbl._y}}}\" for lbl in ax.get_yticklabels()])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14, width=2)\n",
    "        ax.grid(False)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel(r'\\textbf{F1}', fontsize=17)\n",
    "        else:\n",
    "            ax.set_ylabel('', fontsize=0)\n",
    "\n",
    "        ax.set_xlim(0,101)\n",
    "        if ax.get_legend() is not None:\n",
    "            ax.get_legend().remove()\n",
    "        # if model == \"diffusion\" and dataset_name == \"ace05\":\n",
    "        #     handles, labels = ax.get_legend_handles_labels()\n",
    "        #     ax.legend(handles, [\"Original Contamination\", \"original\", \"contam.\", \"clean\"],bbox_to_anchor=(-1.6, -0.3), loc='upper left', fontsize=17, ncol=4)\n",
    "        # else:\n",
    "        #     ax.get_legend().remove()\n",
    "\n",
    "        ax.set_title(model_name_mapping[model], fontsize=17)\n",
    "        ax.set_xlabel(r\"\\textbf{Train Contamination Rate}\", labelpad=20, fontsize=12)\n",
    "\n",
    "    # Set dataset name as title for the first subplot of the row\n",
    "    axs[i].set_title(rf\"\\textbf{{Dataset: {dataset_name_mapping[dataset_name]} / Model: {axs[i].get_title()}}}\", fontsize=17, pad=10)\n",
    "# fig.subplots_adjust(top=0.95, hspace=0.5, wspace=0.2)\n",
    "# fig.suptitle(r\"\\textbf{The effect of entity contamination on NER performance}\")\n",
    "sns.despine()\n",
    "fig.legend(\n",
    "    *axs[0].get_legend_handles_labels(),\n",
    "    bbox_to_anchor=(0.5, -0.1), ncol=4, loc=\"lower center\", fontsize=14)\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"plots/poster_acl2025_performance_per_dataset_and_category.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "9c3a9ab785dc8bdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### plot heatmaps",
   "id": "fb30f27216f57d95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def corr_with_pvalues(data: pd.DataFrame, features: list):\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix and p-value matrix for given features.\n",
    "    \"\"\"\n",
    "    corr_matrix = pd.DataFrame(index=features, columns=features)\n",
    "    pval_matrix = pd.DataFrame(index=features, columns=features)\n",
    "    for col1 in features:\n",
    "        for col2 in features:\n",
    "            if col1 == col2:\n",
    "                corr_matrix.loc[col1, col2] = 1.0\n",
    "                pval_matrix.loc[col1, col2] = 0.0\n",
    "            else:\n",
    "                corr, pval = pearsonr(data[col1], data[col2])\n",
    "                corr_matrix.loc[col1, col2] = corr\n",
    "                pval_matrix.loc[col1, col2] = pval\n",
    "    return corr_matrix.astype(float), pval_matrix.astype(float)\n",
    "\n",
    "\n",
    "def get_statistical_significance_annotations(corr, p_values):\n",
    "    \"\"\"\n",
    "    Prepare annotations for significance levels.\n",
    "    \"\"\"\n",
    "    annotations = np.empty_like(corr, dtype=object)\n",
    "    for i in range(len(corr)):\n",
    "        for j in range(len(corr)):\n",
    "            if i == j:\n",
    "                annotations[i, j] = f\"{corr.iloc[i, j]:.2f}\"\n",
    "            else:\n",
    "                corr_value = f\"{corr.iloc[i, j]:.2f}\"\n",
    "                if p_values.iloc[i, j] < 0.001:\n",
    "                    annotations[i, j] = f\"{corr_value}**\"  # Highly significant\n",
    "                elif p_values.iloc[i, j] < 0.05:\n",
    "                    annotations[i, j] = f\"{corr_value}*\"  # Significant\n",
    "                else:\n",
    "                    annotations[i, j] = corr_value  # Not significant\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(\n",
    "        data: pd.DataFrame,\n",
    "        features: list = [\"num_entities\", \"conta_percent\", \"ent_conta_percent\", \"ner_rec\"],\n",
    "        test_value=None,\n",
    "        pvalue=0.05,\n",
    "        figsize=(7, 7),\n",
    "        cell_size=0.,\n",
    "        xticklabels=[\"Number of\\n Entities\", \"Contamination\\n (%)\", \"Entity Conta-\\nmination (%)\", \"Recall\"],\n",
    "        yticklabels=[\"Number of\\n Entities\", \"Contamination\\n (%)\", \"Entity Conta-\\nmination (%)\", \"Recall\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of correlation coefficients with significance annotations.\n",
    "    \"\"\"\n",
    "    corr, p_values = corr_with_pvalues(data, features)\n",
    "    annotations = get_statistical_significance_annotations(corr, p_values)\n",
    "    significance_mask = p_values > pvalue  # Hide non-significant correlations\n",
    "    dataset_name_mapping = {\"genia\": \"GENIA\", \"nyt\": \"NYT\", \"ace05\": \"ACE05\", \"conll04\": \"CoNLL04\", \"scierc\": \"SciERC\"}\n",
    "    model_name_mapping = {\"iter\": \"ITER\", \"asp\": \"ASP\", \"diffusion\": \"DiffusionNER\"}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        annot=annotations,\n",
    "        fmt='',\n",
    "        cmap='coolwarm',\n",
    "        vmin=-1, vmax=1,\n",
    "        mask=significance_mask,\n",
    "        cbar_kws={\"label\": \"Correlation Coefficient\", \"shrink\": 0.71, },\n",
    "        ax=ax,\n",
    "        square=True,\n",
    "        annot_kws={\"fontsize\": 14},\n",
    "        linewidths=1, linecolor='black'\n",
    "    )\n",
    "    # ax.set_xticks(list(range(0.5, len(features))))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    fontsize = 13\n",
    "    if test_value:\n",
    "        ax.set_title(f'Correlation Heatmap for {model_name_mapping[test_value]} (p < {pvalue})', fontsize=14)\n",
    "    else:\n",
    "        ax.set_title(f'Correlation Heatmap (p < {pvalue})', fontsize=14)\n",
    "    # Set x and y tick labels if provided\n",
    "    if xticklabels:\n",
    "        ax.set_xticklabels(xticklabels, rotation=45, fontsize=fontsize)\n",
    "    else:\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize=fontsize)\n",
    "\n",
    "    if yticklabels:\n",
    "        ax.set_yticklabels(yticklabels, rotation=0, fontsize=fontsize)\n",
    "    else:\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/Correlation{test_value}.pdf\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# normal = x[x[\"test\"] == \"normal\"].rename(columns={\"ner_rec\": \"ner_rec_normal\"})\n",
    "# seen = x[x[\"test\"] == \"seen\"].rename(columns={\"ner_rec\": \"ner_rec_seen\"})\n",
    "# unseen = x[x[\"test\"] == \"unseen\"].rename(columns={\"ner_rec\": \"ner_rec_unseen\"})\n",
    "# # Merge test splits\n",
    "# y = normal.merge(unseen, on=[\"split\", \"conta_percent\", \"dataset\", \"model\"], how=\"left\")\n",
    "# y = y.merge(seen, on=[\"split\", \"conta_percent\", \"dataset\", \"model\"], how=\"left\")\n",
    "# # Compute recall deterioration\n",
    "entire_df[\"ner_rec_delta\"] = entire_df[\"ner_rec\"] - entire_df[\"ner_rec_unseen\"]\n",
    "entire_df[\"diff_seen\"] = entire_df[\"ner_rec_seen\"] - entire_df[\"ner_rec_unseen\"]\n",
    "# Define feature labels\n",
    "# x = pd.concat(dfs)\n",
    "# model2id = {model: idx for idx, model in enumerate(sorted(x[\"model\"].unique()))}\n",
    "# x[\"model_id\"] = x[\"model\"].apply(lambda x: model2id[x])\n",
    "# ticklabels = [\n",
    "#     \"Contamination (%)\",\n",
    "#     \"Recall Delta\\n(orig. - clean)\",\n",
    "#     \"Recall (contam.)\",\n",
    "#     \"Recall (clean)\",\n",
    "#     \"Recall (original)\"\n",
    "# ]\n",
    "ticklabels = [\n",
    "    \"Contamination (%)\",\n",
    "    \"F1 Delta\\n(orig. - clean)\",\n",
    "    \"F1 (contam.)\",\n",
    "    \"F1 (clean)\",\n",
    "    \"F1 (original)\"\n",
    "]\n",
    "features = [\"conta_percent\", \"ner_f1_delta\", \"ner_f1_seen\", \"ner_f1_unseen\", \"ner_f1\"]\n",
    "# features = [\"conta_percent\", \"ner_rec_delta\", \"ner_rec_seen\", \"ner_rec_unseen\", \"ner_rec\"]\n",
    "# Plot heatmap\n",
    "# replace genia diffion nan values with 0.0? might work\n",
    "#  leaving genia out works 100%\n",
    "# tmp = entire_df[~(entire_df.dataset.isin([\"genia\"]) & (entire_df.model==\"diffusion\"))]\n",
    "# print(tmp[tmp.model==\"diffusion\"].dataset.unique())\n",
    "tmp = entire_df[~(entire_df.dataset.isin([\"genia\"]) & (entire_df.model==\"diffusion\") & (entire_df.ner_f1_delta.isna()))].dropna(axis=1, inplace=False)\n",
    "print(tmp[(tmp.dataset.isin([\"genia\"]) & (tmp.model==\"diffusion\"))])\n",
    "plot_correlation_heatmap(tmp, features=features,\n",
    "                         pvalue=0.001, xticklabels=ticklabels, yticklabels=ticklabels, figsize=(7.5, 7.5))\n",
    "for model in tmp.model.unique():\n",
    "    subset = tmp[(tmp['model'] == model)]\n",
    "    plot_correlation_heatmap(subset, features=features, test_value=model, pvalue=0.001,\n",
    "                             xticklabels=ticklabels, yticklabels=ticklabels, figsize=(7.5, 7.5))"
   ],
   "id": "4e49335ebf01cf16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot Boxplots\n",
   "id": "1333f1405cdabd91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_colors = {\n",
    "    \"diffusion\": \"tomato\",\n",
    "    # \"diffusion\": \"lightcoral\",\n",
    "    \"asp\": \"slategray\",\n",
    "    # \"asp\": \"silver\",\n",
    "    \"iter\": \"rebeccapurple\",\n",
    "    # \"iter\": \"mediumpurple\"\n",
    "}\n",
    "label2name = {\"iter\": \"ITER\", \"asp\": \"ASP\", \"diffusion\": \"DiffusionNER\",}\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "})\n",
    "alpha=1\n",
    "fontsize = 12\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "entire_df = entire_df[entire_df[\"conta_percent\"] != 0.0]\n",
    "sns.boxplot(data=entire_df, x=\"conta_percent\", y=\"ner_f1_delta\", palette=model_colors, hue=\"model\", boxprops=dict(alpha=alpha), ax=ax)\n",
    "#sns.stripplot(data=y, x=\"conta_percent\", hue=\"dataset\", palette=\"bright\", y=\"ner_f1_delta\", jitter=0.4, ax=ax)\n",
    "# Calculate and plot regression line\n",
    "unique_x = sorted(entire_df['conta_percent'].unique())\n",
    "x_numeric = np.arange(len(unique_x))\n",
    "line_colors = [\"blue\", \"green\", \"orange\"]\n",
    "y_mean = [entire_df[(entire_df['conta_percent'] == val)]['ner_f1_delta'].mean() for val in unique_x]\n",
    "m, b = np.polyfit(x_numeric, y_mean, 1)\n",
    "\n",
    "ax.plot(x_numeric, m * x_numeric + b, color=\"black\", linestyle='-', linewidth=1.7)\n",
    "ax.grid(True, alpha=0.5, axis=\"y\")\n",
    "ax.set_ylabel(r\"\\textbf{F1 Delta}\", fontsize=14)\n",
    "ax.set_xlabel(r\"\\textbf{Contamination (\\%)}\", fontsize=12)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.set_xticklabels(\n",
    "    [rf\"\\textbf{{{(x+1)*10}}}\" for x in ax.get_xticks()],\n",
    "    fontsize=11\n",
    ")\n",
    "ax.set_yticklabels(\n",
    "    [rf\"\\textbf{{{x}}}\" for x in ax.get_yticks()],\n",
    "    fontsize=fontsize\n",
    ")\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0], labels=[label2name[l] for l in ax.get_legend_handles_labels()[1]], title=r\"\\textbf{Models}\", loc=\"best\")\n",
    "# ax.set_ylim((-2.5, 15))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/F1DeteriorationSeenUnseenByModelAlt.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "####################################################\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "})\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "entire_df = entire_df[entire_df[\"conta_percent\"] != 0.0]\n",
    "sns.boxplot(data=entire_df, x=\"conta_percent\", y=\"ner_rec_delta\",palette=model_colors, hue=\"model\", boxprops=dict(alpha=alpha), ax=ax)\n",
    "# Calculate and plot regression line\n",
    "unique_x = sorted(entire_df['conta_percent'].unique())\n",
    "x_numeric = np.arange(len(unique_x))\n",
    "y_mean = [entire_df[(entire_df['conta_percent'] == val)]['ner_rec_delta'].mean() for val in unique_x]\n",
    "m, b = np.polyfit(x_numeric, y_mean, 1)\n",
    "ax.plot(x_numeric, m * x_numeric + b, color=\"black\", linestyle='-', linewidth=1.7)\n",
    "ax.grid(True, alpha=0.5, axis=\"y\")\n",
    "\n",
    "ax.set_ylabel(r\"\\textbf{Recall Delta}\", fontsize=14)\n",
    "ax.set_xlabel(r\"\\textbf{Contamination (\\%)}\", fontsize=fontsize)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.set_xticklabels(\n",
    "    [rf\"\\textbf{{{(x+1)*10}}}\" for x in ax.get_xticks()],\n",
    "    fontsize=fontsize\n",
    ")\n",
    "ax.set_yticklabels(\n",
    "    [rf\"\\textbf{{{x}}}\" for x in ax.get_yticks()],\n",
    "    fontsize=fontsize\n",
    ")\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0], labels=[label2name[l] for l in ax.get_legend_handles_labels()[1]], title=r\"\\textbf{Models}\", loc=\"best\")\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.title(r\"\\textbf{SOTA-NER-Models Suffer under Higher Contamination}\", fontsize=14 )\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "\n",
    "# ax.set_ylim((-1, 20 ))\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.savefig(\"plots/RecallDeteriorationSeenUnseenByModelAlt.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "42906e460ad75cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Poster version of table",
   "id": "792d95e9c6bd97ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "entire_df = entire_df[entire_df[\"conta_percent\"] != 0.0]\n",
    "\n",
    "model_colors = {\n",
    "    \"diffusion\": \"#de9ed6\",\n",
    "    \"asp\": \"silver\",\n",
    "    \"iter\": \"#ffd6bf\",\n",
    "}\n",
    "half_colors = {\n",
    "    \"diffusion\": \"#a55194\",\n",
    "    \"asp\": \"slategray\",\n",
    "    \"iter\": \"#ffad80\",\n",
    "}\n",
    "\n",
    "label2name = {\"iter\": \"ITER\", \"asp\": \"ASP\", \"diffusion\": \"DiffusionNER\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "})\n",
    "\n",
    "width = 2.6\n",
    "linewidth = 0.001\n",
    "y_axis = \"ner_rec_delta\"\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 6))\n",
    "\n",
    "conta_levels = [i for i in range(10, 110, 10)]\n",
    "bplot = {'boxes': [], 'medians': [], 'whiskers': [], 'caps': [], 'fliers': []}\n",
    "\n",
    "for conta_level in conta_levels:\n",
    "    for i, model in enumerate([\"iter\", \"asp\", \"diffusion\"]):\n",
    "        if i == 0:\n",
    "            positions = [conta_level - width - linewidth]\n",
    "        elif i == 1:\n",
    "            positions = [conta_level]\n",
    "        elif i == 2:\n",
    "            positions = [conta_level + width + linewidth]\n",
    "\n",
    "        tmp_df = entire_df[(entire_df.model == model) & (entire_df.conta_percent == conta_level)]\n",
    "        boxplot = ax.boxplot(\n",
    "            x=-tmp_df[y_axis],\n",
    "            positions=positions,\n",
    "            widths=width,\n",
    "            manage_ticks=False,\n",
    "            patch_artist=True,\n",
    "            label=model,\n",
    "        )\n",
    "\n",
    "        for key in bplot:\n",
    "            bplot[key].extend(boxplot[key])\n",
    "\n",
    "# Set colors for the box plots\n",
    "for patch, color, half_color in zip(\n",
    "    bplot['boxes'],\n",
    "    [model_colors[model] for model in [\"iter\", \"asp\", \"diffusion\"] * len(conta_levels)],\n",
    "    [half_colors[model] for model in [\"iter\", \"asp\", \"diffusion\"] * len(conta_levels)]\n",
    "):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_edgecolor(half_color)\n",
    "\n",
    "# Set median lines to black\n",
    "for median in bplot['medians']:\n",
    "    median.set(color='black')\n",
    "\n",
    "x_numeric = np.array(sorted([int(x) for x in entire_df['conta_percent'].unique()]))\n",
    "y_mean = [-entire_df[(entire_df['conta_percent'] == val)][y_axis].mean() for val in x_numeric]\n",
    "plt.xticks(x_numeric, labels=x_numeric)\n",
    "\n",
    "m, b = np.polyfit(x_numeric, y_mean, 1)\n",
    "ax.plot(x_numeric, m * x_numeric + b, color=\"black\", linestyle='-', linewidth=1.7)\n",
    "ax.grid(False, alpha=0.5, axis=\"y\")\n",
    "ax.set_ylabel(r\"\\textbf{Recall Delta}\", fontsize=14)\n",
    "ax.set_xlabel(r\"\\textbf{Contamination (\\%)}\", fontsize=fontsize)\n",
    "ax.tick_params(axis='both', labelsize=12)\n",
    "ax.set_xticklabels(\n",
    "    [rf\"\\textbf{{{(x)}}}\" for x in ax.get_xticks()],\n",
    "    fontsize=fontsize\n",
    ")\n",
    "ax.set_yticklabels(\n",
    "    [rf\"\\textbf{{{x}}}\" for x in ax.get_yticks()],\n",
    "    fontsize=fontsize\n",
    ")\n",
    "ax.legend(handles=ax.get_legend_handles_labels()[0][:3],\n",
    "          labels=[label2name[l] for l in ax.get_legend_handles_labels()[1][:3]], title=r\"\\textbf{Models}\", loc=\"lower left\", fontsize=fontsize, title_fontsize=fontsize)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# plt.ylim(-0.5,20)\n",
    "# plt.title(r\"\\textbf{SOTA-NER-Models Suffer under Higher Contamination}\", fontsize=14 )\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "\n",
    "# ax.set_ylim((-1, 20 ))\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.savefig(\"plots/RecallDeteriorationSeenUnseenByModelAlt2.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "id": "7acd92a53c6e04e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "data = {\n",
    "    \"ACE05\": {\"DiffusionNER\": {\"F1\": 86.73, \"clean F1\": 77.88}, \"ASP\": {\"F1\": 90.20, \"clean F1\": 83.86},\n",
    "              \"ITER\": {\"F1\": 89.53, \"clean F1\": 82.69}},\n",
    "    \"CoNLL04\": {\"DiffusionNER\": {\"F1\": 87.42, \"clean F1\": 85.15}, \"ASP\": {\"F1\": 88.30, \"clean F1\": 85.11},\n",
    "                \"ITER\": {\"F1\": 91.23, \"clean F1\": 89.21}},\n",
    "    \"GENIA\": {\"DiffusionNER\": {\"F1\": 79.07, \"clean F1\": 75.88}, \"ASP\": {\"F1\": 0, \"clean F1\": 0},\n",
    "              \"ITER\": {\"F1\": 80.82, \"clean F1\": 77.15}},\n",
    "    \"NYT\": {\"DiffusionNER\": {\"F1\": 95.03, \"clean F1\": 87.35}, \"ASP\": {\"F1\": 94.39, \"clean F1\": 79.48},\n",
    "            \"ITER\": {\"F1\": 94.48, \"clean F1\": 82.12}},\n",
    "    \"SciERC\": {\"DiffusionNER\": {\"F1\": 66.26, \"clean F1\": 64.64}, \"ASP\": {\"F1\": 67.86, \"clean F1\": 66.11},\n",
    "               \"ITER\": {\"F1\": 69.24, \"clean F1\": 67.36}}\n",
    "}\n",
    "\n",
    "datasets = [\"GENIA\", \"SciERC\", \"CoNLL04\", \"NYT\", \"ACE05\"]\n",
    "models = [\"ASP\", \"DiffusionNER\", \"ITER\", ]\n",
    "dataset_colors = ['#000000', '#ffffff', '#000000', '#ffffff', '#000000']\n",
    "\n",
    "bar_height = 0.1\n",
    "y = np.arange(len(datasets))\n",
    "model_spacing = 0.05\n",
    "custom_colors = {\n",
    "    \"DiffusionNER F1\": \"tomato\",\n",
    "    \"DiffusionNER clean F1\": \"lightcoral\",\n",
    "    \"ASP F1\": \"slategray\",\n",
    "    \"ASP clean F1\": \"silver\",\n",
    "    \"ITER F1\": \"rebeccapurple\",\n",
    "    \"ITER clean F1\": \"mediumpurple\"\n",
    "}\n",
    "\n",
    "group_height = (len(models) * 2 * bar_height) + ((len(models) - 1) * model_spacing)\n",
    "y = np.arange(len(datasets))\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "})\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "# for idx, dataset in enumerate(datasets):\n",
    "#     start = idx - 0.2\n",
    "#     end = start + (bar_height*2+model_spacing)*3 + 0.2\n",
    "#     ax.axhspan(start, start+0.01, facecolor=dataset_colors[idx], alpha=0.09)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    f1_scores = [data[dataset][model][\"F1\"] for dataset in datasets]\n",
    "    clean_f1_scores = [data[dataset][model][\"clean F1\"] for dataset in datasets]\n",
    "    delta_f1_scores = [f1 - clean_f1 for f1, clean_f1 in zip(f1_scores, clean_f1_scores)]\n",
    "    offset = i * (2 * bar_height + model_spacing)\n",
    "\n",
    "    ax.barh(y + offset, f1_scores, height=bar_height, edgecolor=\"white\", hatch='/', alpha=0.999,\n",
    "            linewidth=0.8, label=f\"{model} F1\", color=custom_colors[f\"{model} F1\"])\n",
    "    ax.barh(y + offset + bar_height, clean_f1_scores, height=bar_height, edgecolor=\"white\",\n",
    "            linewidth=0.8, label=f\"{model} clean F1\", color=custom_colors[f\"{model} clean F1\"])\n",
    "\n",
    "    # Annotate arrows\n",
    "    for j in range(len(datasets)):\n",
    "        f1 = f1_scores[j]\n",
    "        clean_f1 = clean_f1_scores[j]\n",
    "        delta = round(f1 - clean_f1, 2)\n",
    "        if f1 == 0:\n",
    "            continue\n",
    "        mid_y = y[j] + offset + bar_height + bar_height / 2\n",
    "        ax.annotate(\n",
    "            \"\",  # no text\n",
    "            xy=(clean_f1_scores[j] , y[j] - 0.05 + offset + bar_height + bar_height / 2),  # arrow end\n",
    "            xytext=(f1_scores[j], y[j] - 0.05 + offset + bar_height + bar_height / 2),  # arrow start\n",
    "            arrowprops=dict(width=1, headwidth=5, headlength=5,\n",
    "                            facecolor=\"black\", lw=0.1, ))\n",
    "        ax.text(\n",
    "            (f1 + 1.1),  # x midpoint\n",
    "            mid_y - 0.1,  # slightly above arrow\n",
    "            rf\"\\textbf{{{-delta}}}\",\n",
    "            ha='center', va='bottom', fontsize=11, color=\"black\"\n",
    "        )\n",
    "custom_ticks = [0.4, 1.3, 2.3, 3.3, 4.3]  # Define your custom tick positions\n",
    "ax.set_yticklabels([rf\"\\textbf{{{ds}}}\" for ds in datasets], fontsize=12)\n",
    "plt.yticks(custom_ticks)\n",
    "ax.tick_params(axis='y', which='both', left=False)\n",
    "\n",
    "ax.set_title(r\"\\textbf{SOTA-NER-Models perform Worse on Unseen Named Entities}\", fontsize=14)\n",
    "ax.set_xlabel(r\"\\textbf{F1 Score}\", fontsize=12)\n",
    "ax.set_xlim([63, 97])\n",
    "xticks = np.arange(65, 100, 5)\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels([rf\"\\textbf{{{x}}}\" for x in xticks], fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylim([0.1, 4.7])\n",
    "legend_elements = [\n",
    "    Patch(facecolor=custom_colors[\"DiffusionNER F1\"], label=r\"DiffusionNER\"),\n",
    "    Patch(facecolor=custom_colors[\"ASP F1\"], label=r\"ASP\"),\n",
    "    Patch(facecolor=custom_colors[\"ITER F1\"], label=r\"ITER\"),\n",
    "    Patch(facecolor='white', edgecolor='black', hatch='//', label=r\"Normal\\ Testset\"),\n",
    "    Patch(facecolor='white', edgecolor='black', label=r\"Clean\\ Testset\")\n",
    "]\n",
    "# plt.grid(True, linestyle=(0, (5,10)), alpha=0.3,axis=\"both\")\n",
    "ax.legend(handles=legend_elements, loc='best', fontsize=14, title=r\"\\textbf{Models}\", title_fontproperties={\"size\": 14})\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/flo/Downloads/modelperformancedecrease.pdf\", dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "105e57e2882e1b8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot per class counts\n",
   "id": "15f04356b65d14b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 12,\n",
    "    'figure.titlesize': 16\n",
    "})\n",
    "RELATED_COLUMNS = [\"conta_percent\", \"split\", \"ent_conta_percent_normal\"]\n",
    "COL_WRAP_MAPPING = {\"scierc\": 3, \"default\": 4}\n",
    "\n",
    "\n",
    "def prepare_data(df, dataset: str) -> pd.DataFrame:\n",
    "    \"\"\"Prepare and melt data for visualization.\"\"\"\n",
    "    filtered_df = df[df.dataset == dataset]\n",
    "    # Get normal precision per class\n",
    "    metric = \"_pr\" # \"rec\"\n",
    "    filtered_df.columns = filtered_df.columns.str.lower()\n",
    "    norm_pr_cols = [\n",
    "        col for col in filtered_df.columns\n",
    "        if metric in col and not col.split(metric)[0] + \"_\" in [\"\", \"ner_\", \"macro_\", \"macro_ner_\"] and \"unseen\" not in col and \"_seen\" not in col and \"pre\" not in col\n",
    "    ]\n",
    "    # Get clean recall per class\n",
    "    metric = \"rec\"\n",
    "    clean_rec_cols = [\n",
    "        col for col in filtered_df.columns\n",
    "        if metric in col and not col.split(metric)[0] in [\"\", \"ner_\", \"macro_\", \"macro_ner_\"] and \"unseen\" in col\n",
    "    ]\n",
    "    assert len(clean_rec_cols) == len(norm_pr_cols)\n",
    "    melted_data_pr = filtered_df.melt(\n",
    "        id_vars=[\"dataset\", \"split\", \"conta_percent\", \"model\", \"test\"],\n",
    "        value_vars=norm_pr_cols,\n",
    "        var_name=\"ent_type\",\n",
    "        value_name=\"precision\"\n",
    "    )\n",
    "    melted_data_pr[\"ent_type\"] = melted_data_pr[\"ent_type\"].str.split(\"_\").str[0]\n",
    "    melted_data_rec = filtered_df.melt(\n",
    "        id_vars=[\"dataset\", \"split\", \"conta_percent\", \"model\", \"test\"],\n",
    "        value_vars=clean_rec_cols,\n",
    "        var_name=\"ent_type\",\n",
    "        value_name=\"recall\"\n",
    "    )\n",
    "    melted_data_rec[\"ent_type\"] = melted_data_rec[\"ent_type\"].str.split(\"_\").str[0]\n",
    "    melted_data = melted_data_pr.merge(melted_data_rec, on=[\"dataset\", \"split\", \"conta_percent\", \"model\", \"test\", \"ent_type\"])\n",
    "    melted_data[\"clean_f1\"] = (2 * melted_data[\"precision\"] * melted_data[\"recall\"]) / (melted_data[\"precision\"] + melted_data[\"recall\"])\n",
    "    return melted_data\n",
    "\n",
    "\n",
    "def create_facet_grid(data: pd.DataFrame, dataset: str) -> sns.FacetGrid:\n",
    "    \"\"\"Create and configure FacetGrid visualization.\"\"\"\n",
    "    col_wrap = COL_WRAP_MAPPING.get(dataset, COL_WRAP_MAPPING[\"default\"])\n",
    "    sorted_entities = data.groupby(\"ent_type\")[\"count\"].median().sort_values().index\n",
    "\n",
    "    grid = sns.FacetGrid(\n",
    "        data=data,\n",
    "        col=\"ent_type\",\n",
    "        col_order=sorted_entities,\n",
    "        col_wrap=col_wrap,\n",
    "        height=4,\n",
    "        aspect=1.2,\n",
    "    )\n",
    "    #grid.fig.set_size_inches(12, 8)  # Ensuring fixed figure size\n",
    "    # Explicitly set hue normalization\n",
    "    vmin = data[\"count\"].min()\n",
    "    vmax = data[\"count\"].max()\n",
    "    grid.map_dataframe(\n",
    "        sns.scatterplot,\n",
    "        x=\"conta_percent\",\n",
    "        y=\"clean_f1\",\n",
    "        hue=\"count\",\n",
    "        palette=\"rocket\",\n",
    "        hue_norm=(vmin, vmax),  # Explicit normalization\n",
    "        legend=False,\n",
    "        s=50,\n",
    "    )\n",
    "    grid.set(yticks=list(range(round(int(data[\"clean_f1\"].min())//20)*20-20,101, 20)))\n",
    "    grid.set(ylim=(round(int(data[\"clean_f1\"].min()), -1 ), None))\n",
    "\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def add_colorbar(fig, ax, data: pd.DataFrame):\n",
    "    \"\"\"Add standardized colorbar using plot-level min/max.\"\"\"\n",
    "    vmin = data[\"count\"].min()\n",
    "    vmax = data[\"count\"].max()\n",
    "\n",
    "    norm = plt.Normalize(vmin=vmin, vmax=vmax)\n",
    "    sm = plt.cm.ScalarMappable(cmap=\"rocket\", norm=norm)\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, ax=ax, label=\"Class Counts\")\n",
    "\n",
    "\n",
    "def main_visualization_loop(datasets: List[str], x: pd.DataFrame, count_df: pd.DataFrame):\n",
    "    \"\"\"Main loop for generating visualizations.\"\"\"\n",
    "    print(\"Available datasets:\", datasets)\n",
    "\n",
    "    for dataset in datasets:\n",
    "\n",
    "        melted_data = prepare_data(x, dataset)\n",
    "        # Merge with count data\n",
    "        merged_data = pd.merge(\n",
    "            count_df[(count_df.dataset == dataset) & (count_df.dataset_split == \"train\")],\n",
    "            melted_data,\n",
    "            on=[\"dataset\", \"split\", \"conta_percent\", \"ent_type\"]\n",
    "        )\n",
    "        grid = create_facet_grid(merged_data, dataset)\n",
    "        # grid.set_axis_labels(\"Contamination %\", \"Recall\")\n",
    "\n",
    "        # Add colorbar to last subplot\n",
    "        add_colorbar(grid.fig, grid.axes[-1], merged_data)\n",
    "        grid.fig.subplots_adjust(top=0.85)\n",
    "        grid.set_axis_labels(\"Contamination %\", \"F1 clean\", fontsize=14)\n",
    "\n",
    "        grid.set(xticks=[20, 40, 60, 80, 100])\n",
    "        for ax in grid.axes.flat:\n",
    "            ax.tick_params(axis='x', labelsize=12)  # Set x-axis tick font size\n",
    "            ax.tick_params(axis='y', labelsize=12)  # Set y-axis tick font size\n",
    "\n",
    "        grid.set_titles(col_template=\"Entity Type: {col_name}\", fontsize=14)\n",
    "        if dataset in [\"nyt\", \"genia\", \"conll04\",]:\n",
    "            grid.fig.set_size_inches(12,3)\n",
    "            grid.fig.suptitle(f\"Dataset: {dataset_name_mapping[dataset]}\", x=0.4 if dataset in [\"nyt\", \"genia\",] else 0.5, fontsize=15, y=1.03)\n",
    "            grid.fig.subplots_adjust(wspace=0.3)\n",
    "        elif dataset == \"scierc\":\n",
    "            grid.fig.set_size_inches(9,5)\n",
    "            grid.fig.suptitle(f\"Dataset: {dataset_name_mapping[dataset]}\", x=0.4 if dataset in [\"nyt\", \"genia\",] else 0.5, fontsize=15, y=0.95)\n",
    "            grid.fig.subplots_adjust(wspace=0.3, hspace=0.32)\n",
    "        else:\n",
    "            grid.fig.set_size_inches(12,5)\n",
    "            grid.fig.suptitle(f\"Dataset: {dataset_name_mapping[dataset]}\", x=0.4 if dataset in [\"nyt\", \"genia\",] else 0.5, fontsize=15, y=0.97)\n",
    "            grid.fig.subplots_adjust(wspace=0.3)\n",
    "            # Uniform figure size for all datasets\n",
    "        plt.savefig(f\"plots/{dataset}_per_class.pdf\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "datasets = [\"nyt\", \"ace05\", \"genia\", \"conll04\", \"scierc\"]\n",
    "main_visualization_loop(datasets, entire_df, count_df)"
   ],
   "id": "fe58336c163a95ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### compare Original dataset to Mincut by NE Class counts",
   "id": "39e532cacb78cf96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_split2entity_counts(ds_name, train_file=None, dev_file=None, test_file=None):\n",
    "    split2entity_counts = dict()\n",
    "    split2data = get_dataset(\n",
    "        dataset_name=ds_name,\n",
    "        train_file=train_file,\n",
    "        dev_file=dev_file,\n",
    "        test_file=test_file,\n",
    "        type_file=f\"{ds_name}_types.json\",\n",
    "        data_dir=\"datasets/\"\n",
    "    )\n",
    "    for split, df in split2data.items():\n",
    "        if split == \"types\" or df is None:\n",
    "            continue\n",
    "        split = split.split(\"_\")[0]\n",
    "        split2entity_counts[split] = Counter([ent[\"type\"] for example in df[\"entities\"] for ent in example])\n",
    "    if \"dev\" not in split2entity_counts:\n",
    "        split2entity_counts[\"dev\"] = dict()\n",
    "    return split2entity_counts\n",
    "\n",
    "ratio = \"mincut\"\n",
    "dataset2stats = dict()\n",
    "for ds_name in [\"conll03\", \"conll04\", \"scierc\", \"genia\", \"nyt\", \"ace05\",]: #  \"ade\"]:\n",
    "    print(ds_name)\n",
    "    train_file = f\"{ds_name}_train.json\"\n",
    "    dev_file = f\"{ds_name}_dev.json\"\n",
    "    test_file = f\"{ds_name}_test.json\"\n",
    "    dataset2stats[ds_name] = get_split2entity_counts(ds_name, train_file=train_file, dev_file=dev_file, test_file=test_file)\n",
    "\n",
    "    train_file = f\"{ds_name}_train_{ratio}.json\"\n",
    "    dev_file = f\"{ds_name}_dev_{ratio}.json\"\n",
    "    test_file = f\"{ds_name}_test_{ratio}.json\"\n",
    "    dataset2stats[ds_name + \"_fair\"] = get_split2entity_counts(ds_name, train_file, dev_file, test_file)\n"
   ],
   "id": "251e4da3ff3d93fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count Entity types and output if one has to few samples\n",
    "for ds_name, counts in dataset2stats.items():\n",
    "    if \"fair\" in ds_name:\n",
    "        continue\n",
    "    fair_counts = dataset2stats[ds_name+\"_fair\"]\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        if split in fair_counts and split in counts:\n",
    "            if len(counts[split]) != len(fair_counts[split]):\n",
    "                print(\"Missing Types\", ds_name, split)\n",
    "                # print(f\"{sorted(fair_counts[split].keys())},\\n{sorted(counts[split].keys())}\")\n",
    "                # print(counts, fair_counts)\n",
    "            if any([1 if c < 20 else 0 for enttype, c in fair_counts[split].items()]):\n",
    "                print(\"Fair Too few samples?\", ds_name, split)\n",
    "                # print(f\"{sorted(fair_counts[split].keys())},\\n{sorted(counts[split].keys())}\")\n",
    "                print(f\"orig:{counts},\\nfair:{fair_counts}\")\n",
    "            if any([1 if c < 20 else 0 for enttype, c in counts[split].items()]):\n",
    "                print(\"Orig Too few samples?\", ds_name, split)"
   ],
   "id": "1847b773e8dcac9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Barplot for Entity Counts",
   "id": "a29501a72d5fdf43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset2dsname = {\"ace05\": \"ACE05\", \"conll03\":\"CoNLL03\", \"conll04\":\"CoNLL04\", \"ade\":\"ADE\", \"genia\": \"GENIA\", \"nyt\": \"NYT\", \"scierc\": \"SciERC\"}\n",
    "for dataset in dataset2stats.keys():\n",
    "    if \"fair\" in dataset:\n",
    "        continue\n",
    "    # Create df from entity counts for plotting\n",
    "    print(dataset)\n",
    "    df = pd.DataFrame(dataset2stats[dataset]).reset_index().rename(columns={\"index\": 'EntType'})\n",
    "    tmp = [\"test\"] if dataset in [\"genia\", \"ade\"] else [\"dev\", \"test\"]\n",
    "    value_vars = [c for c in df.columns if c in tmp]\n",
    "    entity_counts_df = pd.DataFrame(dataset2stats[dataset]).reset_index().rename(columns={\"index\": 'EntType'}).melt(\n",
    "        id_vars=\"EntType\", value_vars=value_vars, var_name=\"split\", value_name=\"count\"\n",
    "    )\n",
    "    entity_counts_df[\"option\"] = \"original\"\n",
    "    tmp = pd.DataFrame(dataset2stats[dataset + \"_fair\"]).reset_index().rename(columns={\"index\": 'EntType'}).melt(\n",
    "        id_vars=\"EntType\", value_vars=value_vars, var_name=\"split\", value_name=\"count\"\n",
    "    )\n",
    "    tmp[\"option\"] = \"fair\"\n",
    "\n",
    "    entity_counts_df = pd.concat([entity_counts_df, tmp], axis=0)\n",
    "\n",
    "    # Pivot for stacking\n",
    "    stacked_df = entity_counts_df.pivot_table(index=[\"split\", \"option\"], columns=\"EntType\", values=\"count\",\n",
    "                                              aggfunc=\"sum\").fillna(0)\n",
    "    # Define custom x positions\n",
    "    split_order = value_vars if ds_name not in  [\"genia\", \"ade\"] else [\"train\", \"test\"]\n",
    "    option_order = [\"original\", \"fair\"]\n",
    "\n",
    "    x_positions = {}  # Store positions for x-axis\n",
    "    base_x = 0  # Start position\n",
    "\n",
    "    for split in split_order:\n",
    "        for i, option in enumerate(option_order):\n",
    "            x_positions[(split, option)] = base_x + i * 0.45  # Keep \"original\" and \"fair\" close\n",
    "        base_x += 1  # Add gap before next split\n",
    "\n",
    "    stacked_df[\"x_pos\"] = [x_positions[idx] for idx in stacked_df.index]\n",
    "    hatch_patterns = {\"original\": \"\", \"fair\": \"\\\\\"}  # No pattern for \"original\", striped for \"fair\"}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    # Define colors (same for both \"original\" and \"fair\")\n",
    "    #colors = plt.cm.gnuplot(np.linspace(0.03, 1, stacked_df.shape[1] - 1))\n",
    "    colors = [plt.cm.tab20(i) for i in range(10)]\n",
    "    # Plot stacked bars\n",
    "    bottom = np.zeros(len(stacked_df))  # Track bottom for stacking\n",
    "    for idx, ent_type in enumerate(stacked_df.columns[:-1]):  # Exclude x_pos column\n",
    "        for option in option_order:\n",
    "            mask = stacked_df.index.get_level_values(\"option\") == option\n",
    "            ax.bar(\n",
    "                stacked_df.loc[mask, \"x_pos\"],\n",
    "                stacked_df.loc[mask, ent_type],\n",
    "                bottom=bottom[mask],\n",
    "                label=f\"{ent_type} ({option})\" if option == \"original\" else None,  # Avoid duplicate legend\n",
    "                color=colors[idx],\n",
    "                hatch=hatch_patterns[option],\n",
    "                width=0.4,\n",
    "                edgecolor=\"black\",\n",
    "                alpha=0.75 if option == \"fair\" else 1.0,\n",
    "                linewidth=0.4\n",
    "            )\n",
    "\n",
    "        bottom += stacked_df[ent_type]  # Update stacking positions\n",
    "    # Set x-axis labels properly\n",
    "    ax.set_xticks([np.mean([x_positions[(split, \"original\")], x_positions[(split, \"fair\")]]) for split in split_order])\n",
    "    ax.set_xticklabels(split_order)\n",
    "    # ax.set_yticklabels(labels = [math.e**float(item.get_text()) for item in ax.get_yticklabels()])\n",
    "    ax.set_xlabel(\"Splits\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"{dataset2dsname[dataset]}\")\n",
    "\n",
    "    # Legend (Adding a separate fair-pattern example for clarity)\n",
    "    import matplotlib.patches as mpatches\n",
    "\n",
    "    fair_patch = mpatches.Patch(facecolor=\"white\", edgecolor=\"black\", hatch=\"\\\\\\\\\\\\\", label=\"Fair Data\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    if dataset in [\"genia\", \"ace05\",]:\n",
    "        ax.legend(handles + [fair_patch], [l.split()[0] for l in labels] + [\"Clean\"], title=\"Entity Type\",\n",
    "              bbox_to_anchor=(1, 1), loc='upper right')\n",
    "    elif dataset in [\"conll04\", \"conll03\"]:\n",
    "        ax.legend(handles + [fair_patch], [l.split()[0] for l in labels] + [\"Clean\"], title=\"Entity Type\",\n",
    "               bbox_to_anchor=(0.36, 1), loc='upper center')\n",
    "    elif dataset in [\"nyt\", \"ade\"]:\n",
    "        ax.legend(handles + [fair_patch], [l.split()[0] for l in labels] + [\"Clean\"], title=\"Entity Type\",\n",
    "               bbox_to_anchor=(0, 0), loc='lower left')\n",
    "    elif dataset == \"scierc\":\n",
    "        ax.legend(handles + [fair_patch], [l.split()[0] for l in labels] + [\"Clean\"], title=\"Entity Type\",\n",
    "           bbox_to_anchor=(0.25, 1), loc='upper center')\n",
    "    plt.savefig(f\"plots/{dataset2dsname[dataset]}OrigVsFair.pdf\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n"
   ],
   "id": "4620cd13196985db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Plot Original Clean and Contaminated splits",
   "id": "4eb7ff72f480d2cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset2stats = dict()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for ds_name in [\"conll04\", \"scierc\", \"genia\", \"nyt\", \"ace05\", \"conll03\"]:\n",
    "    test_file = f\"{ds_name}_test.json\"\n",
    "    dataset2stats[ds_name] = get_split2entity_counts(ds_name, test_file=test_file)\n",
    "    for conta in [\"seen\", \"unseen\"]:\n",
    "        test_file = f\"{ds_name}_{conta}_test.json\"\n",
    "        dataset2stats[ds_name + f\"_{conta}\"] = get_split2entity_counts(ds_name, test_file=test_file)\n",
    "\n",
    "\n",
    "def dict_to_df(data):\n",
    "    return pd.DataFrame(\n",
    "        [(dset, split, ent, count) for dset, splits in data.items() for split, counter in splits.items() for ent, count\n",
    "         in counter.items()],\n",
    "        columns=[\"dataset\", \"split\", \"EntType\", \"count\"]\n",
    "    )\n",
    "\n",
    "\n",
    "entity_counts_df = dict_to_df(dataset2stats)\n",
    "entity_counts_df[[\"dataset\", \"option\"]] = entity_counts_df['dataset'].str.split('_', n=1, expand=True).fillna(\n",
    "    \"original\")\n",
    "\n",
    "for dataset in [\"conll04\", \"scierc\", \"genia\", \"nyt\", \"ace05\"]:\n",
    "    fig = plt.figure(figsize=(3.5, 2.5))\n",
    "    clean_patch = mpatches.Patch(facecolor=\"white\", edgecolor=\"black\", hatch=\"\\\\\\\\\\\\\", label=\"Clean\")\n",
    "    conta_patch = mpatches.Patch(facecolor=\"white\", edgecolor=\"black\", hatch=\"///\", label=\"Contam.\")\n",
    "\n",
    "    colors = [plt.cm.tab20(i) for i in range(10)]\n",
    "    ax = entity_counts_df[entity_counts_df.dataset == dataset].pivot_table(index=[\"split\", \"option\"], columns=\"EntType\",\n",
    "                                                                           values=\"count\", aggfunc=\"sum\").fillna(\n",
    "        0).plot.bar(stacked=True, color=colors, width=0.7)\n",
    "    ax.set_xticklabels([\"Original\", \"Contam.\", \"Clean\"], rotation=45)\n",
    "    ax.set_xlabel(\"Test sets\")\n",
    "    ax.set_title(f\"{dataset_name_mapping[dataset]}\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    if dataset == \"scierc\":\n",
    "        ax.legend(bbox_to_anchor=(0.29, 0.5),)\n",
    "    # Apply hatch patterns to the entire bars (not just segments)\n",
    "    bars_list = ax.patches\n",
    "    bar_hatches = [\"\", \"/\", \"\\\\\"]\n",
    "    num_bars = len(bars_list) // 3\n",
    "    for i, bar in enumerate(bars_list):\n",
    "        bar.set_hatch(bar_hatches[i % len(bar_hatches)])  # Cycle through hatches\n",
    "    plt.savefig(f\"plots/{dataset}_enttype_seenUnseenComparison.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ],
   "id": "380c552e812b1b88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Compare models metrics with normal training and eval on normal testset + Seen and Unseen",
   "id": "9ba28b673cda8e5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# load full training metric files and display table\n",
   "id": "17526e7e6fb9a8b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "asp_metrics = json.load(open(\"results/experiment_results/full_training/asp_metrics.json\"))\n",
    "iter_metrics = json.load(open(\"results/experiment_results/full_training/iter_metrics.json\"))\n",
    "_metrics = []\n",
    "# load diff metrics\n",
    "for dataset in [\"conll04\", \"scierc\", \"genia\", \"nyt\", \"ace05\"]:\n",
    "    with open(f\"results/experiment_results/full_training/diffusionner/{dataset}/metrics.json\") as f:\n",
    "        json_data = json.load(f)\n",
    "        tmp_metrics = defaultdict(dict)\n",
    "        for metric, score in json_data[\"test_metrics\"].items():\n",
    "            if \"_prec_\" not in metric and \"_rec_\" not in metric and \"_f1_\" not in metric:\n",
    "                continue\n",
    "            if metric.startswith(\"unseen\"):\n",
    "                m = rename_diff_metric(metric.split(\"_\", maxsplit=1)[1])\n",
    "                tmp_metrics[\"unseen\"][m] = round(score / 100, 4)\n",
    "            elif metric.startswith(\"seen\"):\n",
    "                m = rename_diff_metric(metric.split(\"_\", maxsplit=1)[1])\n",
    "                tmp_metrics[\"seen\"][m] = round(score / 100, 4)\n",
    "            else:\n",
    "                m = rename_diff_metric(metric)\n",
    "                tmp_metrics[\"normal\"][m] = round(score / 100, 4)\n",
    "        for testvalue in [\"normal\", \"seen\", \"unseen\"]:\n",
    "            _metrics.append({\n",
    "                \"dataset\": dataset,\n",
    "                \"test\": testvalue,\n",
    "                \"model\": \"diffusion\",\n",
    "                \"metrics\": tmp_metrics[testvalue],\n",
    "            })\n",
    "# rename asp metrics\n",
    "for dataset, scores in asp_metrics.items():\n",
    "    tmp_metrics = defaultdict(dict)\n",
    "    for test_value, metrics in scores.items():\n",
    "        for metric, score in metrics.items():\n",
    "            if \"Precision\" not in metric and \"Recall\" not in metric and \"F1\" not in metric:\n",
    "                continue\n",
    "            m = rename_asp_metric(metric)\n",
    "            tmp_metrics[test_value][m] = round(score / 100, 4)\n",
    "    for testvalue in [\"normal\", \"seen\", \"unseen\"]:\n",
    "        _metrics.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"test\": testvalue,\n",
    "            \"model\": \"asp\",\n",
    "            \"metrics\": tmp_metrics[testvalue],\n",
    "        })\n",
    "\n",
    "# rename iter metrics\n",
    "for dataset, scores in iter_metrics.items():\n",
    "    tmp_metrics = defaultdict(dict)\n",
    "    for test_value, metrics in scores.items():\n",
    "        for metric, score in metrics.items():\n",
    "            tmp_metrics[test_value][metric] = round(score, 4)\n",
    "    for testvalue in [\"normal\", \"seen\", \"unseen\"]:\n",
    "        _metrics.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"test\": testvalue,\n",
    "            \"model\": \"iter\",\n",
    "            \"metrics\": tmp_metrics[testvalue],\n",
    "        })\n",
    "metric_df = pd.DataFrame(_metrics)\n",
    "metric_df = metric_df.join(pd.json_normalize(metric_df['metrics']).dropna(subset=[\"ner_f1\"]))[[\"dataset\", \"test\", \"model\", \"ner_pr\", \"ner_rec\", \"ner_f1\"]]\n",
    "tmp = pd.merge(metric_df[metric_df.test==\"normal\"], metric_df[metric_df.test==\"seen\"],how=\"inner\", on=[\"dataset\", \"model\"], suffixes=(\"\", \"_contam\"))\n",
    "tmp2 = tmp.merge(metric_df[metric_df.test==\"unseen\"], how=\"inner\", on=[\"dataset\", \"model\"], suffixes=(\"\", \"_clean\"))\n",
    "tmp2[\"ner_f1_clean\"] = (2*tmp2.ner_pr*tmp2.ner_rec_clean) / (tmp2.ner_pr + tmp2.ner_rec_clean)\n",
    "tmp2[\"ner_f1_contam\"] = (2*tmp2.ner_pr*tmp2.ner_rec_contam) / (tmp2.ner_pr + tmp2.ner_rec_contam)\n",
    "tmp2[\"ner_f1_delta\"] = tmp2[\"ner_f1\"] - tmp2[\"ner_f1_clean\"]\n",
    "tmp2[\"ner_rec_delta\"] = tmp2[\"ner_rec\"] - tmp2[\"ner_rec_clean\"]\n",
    "tmp2[[\"dataset\", \"test\", \"model\", \"ner_pr\", \"ner_rec\", \"ner_f1\", \"ner_f1_clean\", \"ner_f1_contam\", \"ner_f1_delta\", \"ner_pr\", \"ner_rec\", \"ner_rec_contam\", \"ner_rec_clean\", \"ner_rec_delta\"]]"
   ],
   "id": "57feafd13211aae3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c5cc5f70be524f1b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
